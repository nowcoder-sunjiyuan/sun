{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 领域术语总混淆？教你构建精准术语词库，提升检索一致性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RAG系统构建过程中，术语混淆直接影响信息检索的精准度与生成内容的质量。\n",
    "\n",
    "这主要源于几个方面：\n",
    "- 向量表示\n",
    "- 不同行业、公司乃至同一组织内部，都可能存在相似词汇却拥有截然不同含义的情况\n",
    "\n",
    "这些因素最终导致检索结果偏离预期，大幅降低了答案的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、术语词库构建与维护（Glossary Management）\n",
    "## 1.1 产生术语混淆的原因\n",
    "\n",
    "- 术语多义性\n",
    "- 同义词与近义词\n",
    "- 领域差异\n",
    "- 企业专属术语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 构建术语词库的目标\n",
    "术语词库是整个术语一致性优化体系的核心基础设施。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 术语词库的构建流程\n",
    "- Step 1：收集术语来源\n",
    "- Step 2：标准化术语\n",
    "- Step 3：建立别名映射关系\n",
    "- Step 4：添加上下文信息\n",
    "- Step 5：构建术语索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个功能完善的术语词库应包含以下关键字段，以确保其结构化和可操作性：\n",
    "\n",
    "| 字段名                 | 内容                                                                                   |\n",
    "|------------------------|----------------------------------------------------------------------------------------|\n",
    "| 术语（Term）           | 神经网络                                                                              |\n",
    "| 别名（Synonyms）       | [\"人工神经网络\", \"NN\"]                                                                |\n",
    "| 定义（Definition）     | 神经网络是一种模仿生物神经网络结构和功能的计算模型……                                  |\n",
    "| 上下文标签（Context Tags） | [\"人工智能\", \"深度学习\", \"计算机科学\"]                                                 |\n",
    "| 所属领域（Domain）     | 人工智能                                                                              |\n",
    "| 示例用法（Usage Example） | 在图像识别任务中，我们使用了一个卷积神经网络。                                        |\n",
    "| 外部链接（External Link） | [维基百科链接](https://en.wikipedia.org/wiki/Artificial_neural_network)               |\n",
    "| 禁用词/误导词（Stop Words / Misleading Terms） | [\"神经系统\"（医学中的不同概念）]                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 术语词库与 RAG 集成\n",
    "\n",
    "- 方式一：预处理阶段替换术语\n",
    "- 方式二：检索增强\n",
    "- 方式三：重排序（Re-ranking）\n",
    "- 方式四：后处理解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 术语词库维护\n",
    "1. 术语词库结构设计\n",
    "这是基础，确定词库所需包含的字段和它们之间的关系。\n",
    "\n",
    "2. 自动抽取术语候选\n",
    "利用 NLP 工具从大量文本中自动识别和提取潜在的术语。\n",
    "\n",
    "3. 专家审核与完善\n",
    "领域专家对自动抽取的术语进行人工审核、修正和补充，确保准确性和专业性。\n",
    "\n",
    "4. 构建术语关系图谱\n",
    "如果有需求，可以进一步构建术语之间的层次、关联关系，形成本体（Ontology）或知识图谱（Knowledge Graph），以提升语义理解能力。\n",
    "\n",
    "5. 版本控制与更新机制建设\n",
    "建立术语词库的版本管理和定期更新机制，确保其时效性和权威性，应对新术语的出现或旧术语含义的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 阶段             | 技术名称                                 | 描述                                                                                                                                               |\n",
    "|------------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1. 数据预处理    | 术语抽取、标准化、上下文分块             | 在原始文档和查询进入 RAG 系统之前，识别并提取领域术语，进行统一化处理，并确保文本分块时能有效保留术语的上下文信息。                                   |\n",
    "| 2. 术语词库构建  | 词库设计、术语关系建模、版本管理         | 建立结构化的术语词库，包含术语、别名、定义、上下文标签等字段。进一步可构建术语间的层级或关联关系（如本体），并建立完善的版本控制与更新机制。           |\n",
    "| 3. 嵌入与向量化  | 构建术语向量索引、微调领域嵌入模型       | 将术语词库中的标准术语和别名转换为向量，并构建高效的向量索引（如 FAISS）。同时，通过领域适应性训练（如 LoRA）优化通用嵌入模型，使其更好地理解领域特有概念。 |\n",
    "| 4. 检索增强      | 查询扩展、混合检索、重排序、元数据过滤   | 利用术语词库对用户查询进行扩展（添加别名），结合向量检索与关键词检索（混合检索）。在召回结果后，通过术语匹配度进行重排序，或利用术语作为元数据进行更精确的过滤。 |\n",
    "| 5. 生成控制      | 提示工程、结构化输出、术语验证           | 设计包含术语词库信息的提示词，引导大模型生成更准确的答案。在输出阶段，可强制模型使用词库中的标准术语，并对生成内容进行术语验证，避免出现混淆或不规范表达。     |\n",
    "| 6. 评估与反馈    | 术语一致性指标、LLM-as-a-Judge、用户反馈 | 建立专门的评估指标来衡量 RAG 系统在术语一致性方面的表现。利用大型语言模型作为评估器（LLM-as-a-Judge）来检查术语使用情况，并收集用户反馈，持续优化词库和系统。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 玩具版代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. 数据预处理阶段：术语标准化 ---\n",
      "原始查询: 我想了解一下ML模型在图像识别中的应用，还有NLP的相关知识。\n",
      "标准化查询: 我想了解一下机器学习模型在图像识别中的应用，还有自然语言处理的相关知识。\n",
      "原始文档: 最近研究了CNN和AI算法，发现它们在处理大数据方面表现出色，特别是ML在某些场景下的应用。\n",
      "标准化文档: 最近研究了卷积神经网络和机器学习习，发现它们在处理大数据方面表现出色，特别是机器学习在某些场景下的应用。\n",
      "\n",
      "--- 3. 术语提取 ---\n",
      "从查询中提取的术语: ['机器学习', '自然语言处理']\n",
      "从文档中提取的术语: ['卷积神经网络', '机器学习']\n",
      "\n",
      "--- 4. 模拟向量化存储和检索增强（概念性） ---\n",
      "在实际应用中，我们将使用嵌入模型（例如 SentenceTransformers）将标准化后的文本和术语转换为向量。\n",
      "这些向量随后存储在专门的向量数据库中（如 FAISS、Pinecone 或 Weaviate），以便进行高效的相似性搜索。\n",
      "在检索过程中，用户查询首先被标准化并向量化，然后用于查询向量数据库以获取相关文档。\n",
      "\n",
      "--- 5. 模拟检索增强：查询扩展 ---\n",
      "原始检索查询: 我想知道CPU在电脑里的作用，还有成本CPU是什么？\n",
      "扩展后的检索关键词列表: ['CPU', '中央处理器', '我想知道中央处理器在电脑里的作用，还有成本每单位成本是什么？', '每单位成本']\n",
      "\n",
      "在生产RAG系统中，这些扩展关键词将驱动混合检索策略，结合语义（向量）搜索与基于关键词的搜索，以获得最佳结果。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 1. 定义术语词库 (保持更新，包含上下文标签)\n",
    "GLOSSARY = [\n",
    "    {\"term\": \"卷积神经网络\", \"synonyms\": [\"CNN\", \"卷基神经网络\"], \"definition\": \"一种模仿生物神经网络结构和功能的计算模型，特别适用于图像处理。\", \"context_tags\": [\"图像识别\", \"深度学习\"]},\n",
    "    {\"term\": \"机器学习\", \"synonyms\": [\"ML\", \"机器学\", \"AI算法\"], \"definition\": \"一种人工智能领域，使计算机系统能够从数据中学习而无需明确编程。\", \"context_tags\": [\"人工智能\", \"数据科学\"]},\n",
    "    {\"term\": \"自然语言处理\", \"synonyms\": [\"NLP\", \"自然语言\"], \"definition\": \"研究人类语言和计算机之间交互的领域。\", \"context_tags\": [\"人工智能\", \"语言学\"]},\n",
    "    {\"term\": \"中央处理器\", \"synonyms\": [\"CPU\"], \"definition\": \"计算机的算术、逻辑和控制单元。\", \"context_tags\": [\"计算机硬件\", \"电脑\"]},\n",
    "    {\"term\": \"每单位成本\", \"synonyms\": [\"CPU\"], \"definition\": \"业务分析中衡量每个产品或服务单位的成本。\", \"context_tags\": [\"业务分析\", \"财务管理\", \"成本\"]},\n",
    "]\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"构建映射，一个别名可以映射到多个术语条目以处理歧义。\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "            \n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        使用迭代和替换函数进行上下文感知的术语标准化。\n",
    "        为不同类型的术语动态生成正确的正则表达式。\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "        \n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "            \n",
    "            # --- 动态为每个key创建正确的正则表达式 ---\n",
    "            pattern_str = \"\"\n",
    "            # 如果key包含英文字母，我们假定它是一个缩写，需要边界判断\n",
    "            if re.search(r'[a-zA-Z]', key_lower):\n",
    "                 # 使用环视来确保不匹配单词内部\n",
    "                 pattern_str = r'(?<![a-zA-Z])' + re.escape(key_lower) + r'(?![a-zA-Z])'\n",
    "            else:\n",
    "                 # 对于中文术语，直接精确匹配\n",
    "                 pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # 定义一个替换函数，在每次匹配时调用\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- 上下文消歧逻辑 ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window):\n",
    "                        min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # 如果没有上下文线索，回退到第一个定义\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # 使用替换函数更新文本\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        从文本中提取已知的标准术语\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # 直接进行字符串搜索，不使用 \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "                \n",
    "        return sorted(list(found_terms))\n",
    "\n",
    "\n",
    "# 1. 使用词库初始化术语处理器。\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "\n",
    "# 2. 数据预处理阶段：术语标准化\n",
    "print(\"--- 2. 数据预处理阶段：术语标准化 ---\")\n",
    "user_query = \"我想了解一下ML模型在图像识别中的应用，还有NLP的相关知识。\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"原始查询: {user_query}\")\n",
    "print(f\"标准化查询: {processed_query}\")\n",
    "\n",
    "document_text = \"最近研究了CNN和AI算法，发现它们在处理大数据方面表现出色，特别是ML在某些场景下的应用。\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"原始文档: {document_text}\")\n",
    "print(f\"标准化文档: {processed_document}\")\n",
    "\n",
    "# 3. 术语提取（用于后续的向量化或元数据标记）\n",
    "print(\"\\n--- 3. 术语提取 ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"从查询中提取的术语: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"从文档中提取的术语: {extracted_terms_document}\")\n",
    "\n",
    "# 4. 模拟向量化存储和检索增强（概念性）\n",
    "print(\"\\n--- 4. 模拟向量化存储和检索增强（概念性） ---\")\n",
    "print(\"在实际应用中，我们将使用嵌入模型（例如 SentenceTransformers）将标准化后的文本和术语转换为向量。\")\n",
    "print(\"这些向量随后存储在专门的向量数据库中（如 FAISS、Pinecone 或 Weaviate），以便进行高效的相似性搜索。\")\n",
    "print(\"在检索过程中，用户查询首先被标准化并向量化，然后用于查询向量数据库以获取相关文档。\")\n",
    "\n",
    "# 5. 模拟检索增强：查询扩展\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"通过术语词库扩展查询关键词以提高召回率。\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "    \n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))\n",
    "\n",
    "print(\"\\n--- 5. 模拟检索增强：查询扩展 ---\")\n",
    "original_query_for_retrieval = \"我想知道CPU在电脑里的作用，还有成本CPU是什么？\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"原始检索查询: {original_query_for_retrieval}\")\n",
    "print(f\"扩展后的检索关键词列表: {expanded_keywords}\")\n",
    "\n",
    "print(\"\\n在生产RAG系统中，这些扩展关键词将驱动混合检索策略，结合语义（向量）搜索与基于关键词的搜索，以获得最佳结果。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据预处理阶段（Preprocessing）：提升语义表示质量\n",
    "\n",
    "这是术语一致性优化的“第一道防线”，直接影响后续所有环节的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 技术名称                          | 描述                                       | 对术语一致性的帮助                           |\n",
    "|-----------------------------------|--------------------------------------------|----------------------------------------------|\n",
    "| 术语抽取（NER、TF-IDF、KeyBERT）  | 自动从语料中识别候选术语                   | 提供术语来源，是词库构建的基础               |\n",
    "| 术语标准化（Term Normalization）  | 替换非标准表达为统一术语（如“AI”→“人工智能”） | 消除输入噪声，确保术语表达一致               |\n",
    "| 文本清洗与格式统一                | 清理无意义内容、统一大小写、标点等         | 减少干扰，提升术语识别准确率                 |\n",
    "| 上下文感知分块策略（SemanticChunker） | 按语义相似度切分文本块                     | 保留术语所在上下文信息，避免割裂语义         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境准备\n",
    "首先，确保你安装了必要的Python库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers sentence-transformers faiss-cpu scikit-learn spacy\n",
    "! python -m spacy download zh_core_web_sm\n",
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤一：术语词库结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个结构化的JSON，定义了我们知识体系的核心\n",
    "term_glossary = {\n",
    "    \"神经网络\": {\n",
    "        \"synonyms\": [\"人工神经网络\", \"NN\"],\n",
    "        \"definition\": \"模仿生物神经网络结构和功能的计算模型\",\n",
    "        \"context_tags\": [\"人工智能\", \"深度学习\"],\n",
    "        \"domain\": \"计算机科学\",\n",
    "        \"stop_words\": [\"神经系统\"]\n",
    "    },\n",
    "    \"卷积神经网络\": {\n",
    "        \"synonyms\": [\"CNN\", \"ConvNet\"],\n",
    "        \"definition\": \"通过卷积层提取局部特征的深度学习模型\",\n",
    "        \"context_tags\": [\"计算机视觉\", \"图像识别\"],\n",
    "        \"domain\": \"人工智能\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"图像识别\": { \n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"使计算机识别图像内容的任务\",\n",
    "        \"context_tags\": [\"计算机视觉\"],\n",
    "        \"domain\": \"人工智能\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"自动驾驶\": { \n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"使车辆无需人工操作即可自主行驶的技术\",\n",
    "        \"context_tags\": [\"人工智能\", \"机器人\"],\n",
    "        \"domain\": \"计算机科学\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"医疗影像诊断\": {\n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"利用医学影像进行疾病诊断\",\n",
    "        \"context_tags\": [\"医疗\", \"图像处理\"],\n",
    "        \"domain\": \"医学\",\n",
    "        \"stop_words\": []\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.1 术语抽取与标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自动抽取的术语候选: {'医疗影像诊断', '图像识别', '自动驾驶', 'CNN'}\n"
     ]
    }
   ],
   "source": [
    "# 使用spaCy的EntityRuler，根据我们的词库自定义实体识别规则\n",
    "import spacy\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"zh_core_web_sm\")\n",
    "    \n",
    "    # 创建一个实体规则管道，并从词库中加载所有术语和别名\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = []\n",
    "    for term, data in glossary.items():\n",
    "        patterns.append({\"label\": \"TERM\", \"pattern\": term})\n",
    "        for syn in data.get(\"synonyms\", []):\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": syn})\n",
    "    ruler.add_patterns(patterns)\n",
    "    \n",
    "    # 处理文本并提取被识别为\"TERM\"的实体\n",
    "    doc = nlp(text)\n",
    "    candidates = {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}\n",
    "    return candidates\n",
    "\n",
    "# 来看一个例子\n",
    "text = \"CNN模型在图像识别中的应用案例包括自动驾驶和医疗影像诊断。\"\n",
    "candidates = extract_terms_with_ruler(text, term_glossary)\n",
    "print(f\"自动抽取的术语候选: {candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、嵌入构建与向量化阶段（Embedding & Vectorization）\n",
    "\n",
    "核心任务是将这些经过清洗和标准化的术语，转化为机器能够理解和计算的密集向量（Dense Vectors），并构建高效的检索索引。这直接决定了系统语义匹配的能力上限。\n",
    "\n",
    "| 技术名称                                 | 描述                                       | 对术语一致性的帮助                             |\n",
    "|------------------------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| 术语嵌入与向量索引（FAISS / Pinecone）   | 将术语及其别名转换为向量并构建索引         | 支持语义匹配，提升检索时的术语识别能力         |\n",
    "| 域专用嵌入模型（Legal-BERT、ChatLaw-Text2Vec） | 在专业语料上继续训练通用模型               | 提升术语理解质量，增强语义表示                 |\n",
    "| Sentence Transformers + PEFT（LoRA）微调 | 参数高效微调嵌入模型                       | 针对特定领域进一步优化术语语义表示             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.2 基于向量相似度的同义词发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "优化后匹配的别名: {'卷积神经网络': ['人工神经网络', '神经系统', '深度学习模型'], '神经网络': ['人工神经网络', '神经系统']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 建议在项目初始化时加载模型，避免重复加载\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2') \n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    通过计算向量余弦相似度，将候选词映射到最接近的标准术语。\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): 标准术语列表。\n",
    "        candidates (list): 待匹配的候选同义词列表。\n",
    "        threshold (float): 判断为同义词的相似度阈值。\n",
    "\n",
    "    Returns:\n",
    "        dict: 映射成功的同\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "\n",
    "    # 批量编码以提升效率\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # 计算标准术语与所有候选词的余弦相似度矩阵\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "    \n",
    "    return _matched_synonyms\n",
    "\n",
    "# 示例：\n",
    "main_terms_to_map = [\"卷积神经网络\", \"神经网络\"]\n",
    "all_possible_synonyms = [\"CNN\", \"ConvNet\", \"人工神经网络\", \"NN\", \"神经系统\", \"深度学习模型\"]\n",
    "\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(main_terms_to_map, all_possible_synonyms)\n",
    "print(\"\\n优化后匹配的别名:\", optimized_mapped_synonyms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 步骤三：构建术语向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代理环境变量已设置。\n",
      "HTTP_PROXY: http://127.0.0.1:4780\n",
      "HTTPS_PROXY: http://127.0.0.1:4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\learnRAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在尝试加载模型 'paraphrase-multilingual-MiniLM-L12-v2'...\n",
      "模型加载/下载成功！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义你的代理地址\n",
    "proxy_url = 'http://127.0.0.1:4780'\n",
    "\n",
    "# 为HTTP和HTTPS流量设置代理\n",
    "# 大多数模型下载等操作都通过HTTPS，所以 'HTTPS_PROXY' 至关重要\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "print(\"代理环境变量已设置。\")\n",
    "print(f\"HTTP_PROXY: {os.getenv('HTTP_PROXY')}\")\n",
    "print(f\"HTTPS_PROXY: {os.getenv('HTTPS_PROXY')}\")\n",
    "\n",
    "# --- 在这里开始你的正常代码 ---\n",
    "# 例如，现在加载模型，它将通过指定的代理进行下载\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 只有在模型未被缓存时，才会通过代理下载\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "print(f\"\\n正在尝试加载模型 '{model_name}'...\")\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"模型加载/下载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"加载模型时出错: {e}\")\n",
    "    print(\"请检查您的代理服务是否正在运行，并且地址和端口是否正确。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成术语向量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS索引构建完成。包含 9 个向量，维度为 384。\n",
      "\n",
      "--- 索引构建成功 ---\n",
      "FAISS 索引中的向量数量: 9\n",
      "被索引的术语列表: ['CNN', 'ConvNet', 'TRANSFORMER', 'Transformer', 'transformer', '卷积神经网络', '图像分类', '图像识别', '视觉识别']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def build_term_vector_index(term_glossary: dict, model: SentenceTransformer) -> tuple:\n",
    "    \"\"\"\n",
    "    将术语词库中的所有术语及其别名转换为向量，并构建FAISS索引。\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict): 结构化的术语词库。键为标准术语，值为包含'synonyms'列表的字典。\n",
    "        model (SentenceTransformer): 已加载的SentenceTransformer模型实例。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (faiss.Index, list) 返回构建好的FAISS索引对象和与之对应的术语列表。\n",
    "    \"\"\"\n",
    "    terms_to_index = []\n",
    "    # 遍历术语映射字典，收集所有标准术语和别名\n",
    "    # 修正点：将 term_mapping_dict 修改为 term_glossary\n",
    "    for standard_term, info in term_glossary.items():\n",
    "        terms_to_index.append(standard_term)\n",
    "        if \"synonyms\" in info and isinstance(info[\"synonyms\"], list):\n",
    "            terms_to_index.extend(info[\"synonyms\"])\n",
    "    \n",
    "    unique_terms_to_index = sorted(list(set(terms_to_index)))\n",
    "    \n",
    "    print(\"正在生成术语向量...\")\n",
    "    embeddings = model.encode(unique_terms_to_index, show_progress_bar=True)\n",
    "    \n",
    "    embeddings = embeddings.astype('float32')\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"FAISS索引构建完成。包含 {index.ntotal} 个向量，维度为 {dimension}。\")\n",
    "    return index, unique_terms_to_index\n",
    "\n",
    "\n",
    "# 1. 加载模型\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') \n",
    "\n",
    "# 2. 准备术语数据\n",
    "term_mapping_example = {\n",
    "    \"卷积神经网络\": {\"synonyms\": [\"CNN\", \"ConvNet\"]},\n",
    "    \"Transformer\": {\"synonyms\": [\"transformer\", \"TRANSFORMER\"]},\n",
    "    \"图像识别\": {\"synonyms\": [\"图像分类\", \"视觉识别\"]}\n",
    "}\n",
    "\n",
    "# 3. 使用修正后的函数进行调用\n",
    "faiss_index, indexed_term_list = build_term_vector_index(term_mapping_example, model)\n",
    "\n",
    "# 4. 验证结果\n",
    "print(\"\\n--- 索引构建成功 ---\")\n",
    "print(f\"FAISS 索引中的向量数量: {faiss_index.ntotal}\")\n",
    "print(f\"被索引的术语列表: {indexed_term_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何调用该函数并获取索引和术语列表的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: 'CNN'\n",
      "检索结果:\n",
      "  Top 1: 术语='CNN',  距离=0.0000 (值越小越相似)\n",
      "  Top 2: 术语='图像分类',  距离=35.1526 (值越小越相似)\n",
      "  Top 3: 术语='ConvNet',  距离=35.1713 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '计算机视觉'\n",
      "检索结果:\n",
      "  Top 1: 术语='视觉识别',  距离=11.4270 (值越小越相似)\n",
      "  Top 2: 术语='图像识别',  距离=14.5564 (值越小越相似)\n",
      "  Top 3: 术语='图像分类',  距离=15.7114 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '语言模型'\n",
      "检索结果:\n",
      "  Top 1: 术语='图像分类',  距离=24.3401 (值越小越相似)\n",
      "  Top 2: 术语='卷积神经网络',  距离=26.5247 (值越小越相似)\n",
      "  Top 3: 术语='图像识别',  距离=27.9374 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '变换器模型'\n",
      "检索结果:\n",
      "  Top 1: 术语='TRANSFORMER',  距离=11.7908 (值越小越相似)\n",
      "  Top 2: 术语='图像分类',  距离=17.0967 (值越小越相似)\n",
      "  Top 3: 术语='卷积神经网络',  距离=17.4776 (值越小越相似)\n"
     ]
    }
   ],
   "source": [
    "# --- 第2部分：定义我们的核心检索函数 ---\n",
    "\n",
    "def search_similar_terms(query_text: str, model: SentenceTransformer, index: faiss.Index, term_list: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    在FAISS索引中检索与查询文本最相似的k个术语。\n",
    "\n",
    "    Args:\n",
    "        query_text (str): 用户输入的查询词。\n",
    "        model (SentenceTransformer): 用于编码查询词的模型。\n",
    "        index (faiss.Index): FAISS索引对象。\n",
    "        term_list (list): 与索引向量顺序一致的术语列表。\n",
    "        k (int): 希望返回的最相似结果的数量。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 正在执行检索 ---\")\n",
    "    print(f\"查询: '{query_text}'\")\n",
    "    \n",
    "    # 1. 将查询文本编码为向量\n",
    "    query_vector = model.encode([query_text])\n",
    "    query_vector = query_vector.astype('float32')\n",
    "    \n",
    "    # 2. 在FAISS索引中执行搜索\n",
    "    # index.search返回两个数组：D (distances) 和 I (indices)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # 3. 解析并打印结果\n",
    "    print(\"检索结果:\")\n",
    "    for i in range(k):\n",
    "        idx = indices[0][i]\n",
    "        dist = distances[0][i]\n",
    "        term = term_list[idx]\n",
    "        \n",
    "        # 对于IndexFlatL2，距离是平方欧氏距离，距离越小代表越相似\n",
    "        print(f\"  Top {i+1}: 术语='{term}',  距离={dist:.4f} (值越小越相似)\")\n",
    "\n",
    "\n",
    "# 4. === 演示效果 ===\n",
    "\n",
    "# **案例一：用别名查询标准术语**\n",
    "# 目标：测试系统能否理解 \"CNN\" 就是 \"卷积神经网络\"。\n",
    "search_similar_terms(query_text=\"CNN\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例二：语义相近查询（核心优势展示）**\n",
    "# 目标：查询一个不在我们词库中，但意思相近的词 \"计算机视觉\"。\n",
    "# 预期：系统应该能找到 \"图像识别\" 或 \"视觉识别\" 等相关术语。\n",
    "search_similar_terms(query_text=\"计算机视觉\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例三：用一个更宽泛的词查询**\n",
    "# 目标：查询 \"语言模型\"，看是否能找到更具体的 \"大型语言模型\" 或 \"自然语言处理\"。\n",
    "search_similar_terms(query_text=\"语言模型\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例四：测试对轻微噪声的容忍度**\n",
    "# 目标：查询一个不存在的、略有差异的词 \"变换器模型\"，看是否能正确匹配到 \"Transformer模型\"。\n",
    "search_similar_terms(query_text=\"变换器模型\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、检索增强阶段\n",
    "\n",
    "核心目标是在初步召回（Recall）的基础上，进一步优化检索结果的广度与精度。预处理阶段解决了术语的“标准”问题，而本阶段则聚焦于如何利用这些标准化的知识，在实际检索中发挥最大效用。\n",
    "\n",
    "| 技术名称                             | 描述                                                                                   | 对术语一致性的帮助                                                                 |\n",
    "|--------------------------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| 查询扩展与重写（MultiQueryRetriever） | 利用 LLM 生成多个语义等价的查询变体，合并检索结果。                                     | 自动覆盖用户未提及的同义词或相关表达，极大提升对多样化术语的识别与召回能力。         |\n",
    "| HyDE（假设文档嵌入）                 | 利用 LLM 为查询生成一个“理想答案”的假设性文档，再用该文档的嵌入进行检索。               | 通过生成富含上下文的理想答案，有效缓解原始查询中术语模糊或信息不足的问题，提升检索相关性。 |\n",
    "| 混合检索（BM25 + FAISS）             | 结合关键词检索（如 BM25）与向量检索（如 FAISS）的优势。                                 | 综合利用字面精确匹配和语义相似匹配，确保基础术语不丢失，同时发现语义相关内容。         |\n",
    "| 交叉编码器重排序（BGE-reranker）     | 使用更复杂的交叉编码器模型（如 BGE-reranker）对召回结果进行精细化重排序。               | 通过深度交互分析查询与文档的匹配度，提升对术语匹配度的排序精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community\n",
    "! pip install langchain langchain-openai faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 核心技术一：查询扩展与重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始查询: CNN是什么？\n",
      "\n",
      "--- MultiQueryRetriever 生成的查询变体 ---\n",
      "查询变体 1: 卷积神经网络的定义是什么？\n",
      "查询变体 2: CNN模型在深度学习中的作用是什么？\n",
      "查询变体 3: 介绍一下卷积神经网络（CNN）。\n",
      "\n",
      "--- 最终检索到的文档内容 ---\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
      "它的核心在于通过卷积层和池化层自动提取图像的局部特征。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
      "例如机器翻译。如今，它也被成功应用于计算机视觉，称为Vision Transformer。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它基于Transformer架构，\n",
      "能够理解和生成类似人类的文本，展现出强大的推理能力。\n"
     ]
    }
   ],
   "source": [
    "## 查询扩展与重写\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# --- 准备工作：设置API Key并创建向量数据库 ---\n",
    "\n",
    "# 设置您的OpenAI API Key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# 1. 准备示例文档\n",
    "# 我们创建一些包含专业术语的示例文本\n",
    "doc_text = \"\"\"\n",
    "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
    "它的核心在于通过卷积层和池化层自动提取图像的局部特征。\n",
    "\n",
    "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
    "例如机器翻译。如今，它也被成功应用于计算机视觉，称为Vision Transformer。\n",
    "\n",
    "大型语言模型（LLM）是当前AI研究的热点，它基于Transformer架构，\n",
    "能够理解和生成类似人类的文本，展现出强大的推理能力。\n",
    "\"\"\"\n",
    "with open(\"sample_tech_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# 2. 加载和切分文档\n",
    "loader = TextLoader(\"sample_tech_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. 创建向量数据库\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# --- MultiQueryRetriever 实现 ---\n",
    "\n",
    "# 4. 初始化LLM和检索器\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# 5. 执行查询\n",
    "query = \"CNN是什么？\"\n",
    "retrieved_docs = retriever_from_llm.invoke(query)\n",
    "\n",
    "# --- 效果分析 ---\n",
    "print(f\"原始查询: {query}\")\n",
    "print(\"\\n--- MultiQueryRetriever 生成的查询变体 ---\")\n",
    "# MultiQueryRetriever 内部有日志记录生成的查询，这里我们手动展示其可能生成的查询\n",
    "# 实际使用中可以通过设置 logging.basicConfig(level=logging.INFO) 查看\n",
    "generated_queries = [\n",
    "    \"卷积神经网络的定义是什么？\",\n",
    "    \"CNN模型在深度学习中的作用是什么？\",\n",
    "    \"介绍一下卷积神经网络（CNN）。\"\n",
    "]\n",
    "for i, q in enumerate(generated_queries):\n",
    "    print(f\"查询变体 {i+1}: {q}\")\n",
    "\n",
    "print(\"\\n--- 最终检索到的文档内容 ---\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 假设性文档嵌入 (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade langchain langchain-community langchain-openai rank_bm25 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档已被切分为 3 个块。\n",
      "\n",
      "正在构建FAISS向量检索器...\n",
      "FAISS检索器构建完成。\n",
      "\n",
      "正在构建BM25关键词检索器...\n",
      "BM25检索器构建完成。\n",
      "\n",
      "正在初始化 MergerRetriever...\n",
      "MergerRetriever 初始化完成。\n",
      "\n",
      "\n",
      "--- 正在执行混合检索 ---\n",
      "查询: 'ViT的技术细节'\n",
      "\n",
      "--- 单独检索结果对比 ---\n",
      "【BM25 关键词检索结果】(共 3 条):\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "\n",
      "【FAISS 向量检索结果】(共 3 条):\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "\n",
      "--- MergerRetriever 混合检索结果 ---\n",
      "【最终混合结果】(共 6 条，已去重):\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# --- 1. 准备工作：设置API Key并准备数据 ---\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# 准备示例文档，我们稍微丰富一下内容以便于对比\n",
    "doc_text = \"\"\"\n",
    "第一部分：关于卷积网络。\n",
    "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
    "它的核心在于通过卷积层和池化层自动提取图像的局部特征。CNN的这个特性让它非常高效。\n",
    "\n",
    "第二部分：关于Transformer。\n",
    "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
    "例如机器翻译。如今，一种被称为Vision Transformer（ViT）的变体也被成功应用于计算机视觉领域。\n",
    "\n",
    "第三部分：关于大模型。\n",
    "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer架构，\n",
    "能够理解和生成类似人类的文本，展现出强大的推理能力。\n",
    "\"\"\"\n",
    "with open(\"hybrid_search_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# 加载和切分文档\n",
    "loader = TextLoader(\"hybrid_search_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"文档已被切分为 {len(docs)} 个块。\")\n",
    "\n",
    "\n",
    "# --- 2. 构建两个不同的检索器 ---\n",
    "\n",
    "# **检索器一：FAISS 向量检索器 (用于语义匹配)**\n",
    "print(\"\\n正在构建FAISS向量检索器...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"FAISS检索器构建完成。\")\n",
    "\n",
    "\n",
    "# **检索器二：BM25 关键词检索器 (用于精确匹配)**\n",
    "print(\"\\n正在构建BM25关键词检索器...\")\n",
    "# BM25Retriever可以直接从文档列表初始化，它不需要嵌入模型\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 3\n",
    "print(\"BM25检索器构建完成。\")\n",
    "\n",
    "\n",
    "# --- 3. 使用 MergerRetriever 合并 ---\n",
    "\n",
    "print(\"\\n正在初始化 MergerRetriever...\")\n",
    "# 创建一个检索器列表\n",
    "retriever_list = [bm25_retriever, faiss_retriever]\n",
    "\n",
    "# 初始化 MergerRetriever\n",
    "# 它会自动处理并行检索和结果去重\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever 初始化完成。\")\n",
    "\n",
    "\n",
    "# --- 4. 执行查询并对比效果 ---\n",
    "\n",
    "query = \"ViT的技术细节\"\n",
    "print(f\"\\n\\n--- 正在执行混合检索 ---\")\n",
    "print(f\"查询: '{query}'\")\n",
    "\n",
    "\n",
    "# **为了对比，我们先看看每个检索器单独工作的结果**\n",
    "print(\"\\n--- 单独检索结果对比 ---\")\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "print(f\"【BM25 关键词检索结果】(共 {len(bm25_results)} 条):\")\n",
    "for doc in bm25_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "faiss_results = faiss_retriever.invoke(query)\n",
    "print(f\"\\n【FAISS 向量检索结果】(共 {len(faiss_results)} 条):\")\n",
    "for doc in faiss_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "\n",
    "# **现在看看 MergerRetriever 的混合结果**\n",
    "print(\"\\n--- MergerRetriever 混合检索结果 ---\")\n",
    "merged_results = merged_retriever.invoke(query)\n",
    "print(f\"【最终混合结果】(共 {len(merged_results)} 条，已去重):\")\n",
    "for doc in merged_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、生成控制与输出验证阶段\n",
    "\n",
    "| 技术名称                  | 描述                                                                 | 对术语一致性的贡献与作用                                                                 |\n",
    "|---------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| 提示工程 (Prompt Engineering)       | 在提示中明确指令，引导 LLM 使用标准术语、保持特定风格。                | 最基础的控制手段，直接影响 LLM 的选词倾向，引导其遵循术语规范。                           |\n",
    "| 结构化输出 (Structured Output)     | 强制 LLM 返回符合预定义模式（如 Pydantic 或 JSON Schema）的对象。      | 从根本上杜绝术语的随意使用，确保关键信息以标准、可控的格式输出。                         |\n",
    "| 输出解析与修复 (Output Parsers)    | 使用如 OutputFixingParser 等工具，在 LLM 输出格式错误时自动尝试修复。  | 提升结构化输出的鲁棒性，能自动纠正轻微的术语格式或拼写错误。                               |\n",
    "| 后处理与内容增强                   | 在答案文本中自动高亮术语、添加定义弹窗或引用链接。                    | 提升最终答案的可读性和专业性，为用户提供即时的术语解释和来源追溯。                        |\n",
    "| LLM 即评委 (LLM-as-a-Judge)        | 使用另一个 LLM 实例，根据预设标准（如术语一致性）对生成结果进行打分评估。 | 提供一种可扩展的、自动化的输出质量与术语合规性评估方案。                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 生成时控制：结构化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在执行结构化输出链 ---\n",
      "\n",
      "--- LLM返回的结构化对象 ---\n",
      "answer='卷积神经网络（Convolutional Neural Network, CNN）是一种深度学习模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\\n\\n主要应用领域包括：\\n\\n1. **图像识别**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\\n2. **目标检测**：用于在图像中定位和识别多个对象。\\n3. **图像分割**：将图像划分为不同的区域或对象。\\n4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\\n5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\\n6. **自动驾驶**：用于识别道路标志、行人和其他车辆。' standard_terms_used=['卷积神经网络', '图像识别', '目标检测', '图像分割', '自然语言处理', '医学影像分析', '自动驾驶']\n",
      "\n",
      "--- 对结果的分析 ---\n",
      "回答内容: 卷积神经网络（Convolutional Neural Network, CNN）是一种深度学习模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\n",
      "\n",
      "主要应用领域包括：\n",
      "\n",
      "1. **图像识别**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\n",
      "2. **目标检测**：用于在图像中定位和识别多个对象。\n",
      "3. **图像分割**：将图像划分为不同的区域或对象。\n",
      "4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\n",
      "5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\n",
      "6. **自动驾驶**：用于识别道路标志、行人和其他车辆。\n",
      "模型确认使用的标准术语: ['卷积神经网络', '图像识别', '目标检测', '图像分割', '自然语言处理', '医学影像分析', '自动驾驶']\n",
      "验证通过：答案中正确地使用了标准术语'卷积神经网络'。\n",
      "\n",
      "==================================================\n",
      "--- 开始执行内容增强 ---\n",
      "\n",
      "--- 内容增强后的最终输出 (在支持HTML的Markdown渲染器中查看) ---\n",
      "<abbr title=\"一种特殊的<abbr title=\"机器学习的一个分支，它基于人工神经网络。\">深度学习</abbr>模型，擅长处理图像数据。\">卷积神经网络</abbr>（Convolutional Neural Network, CNN）是一种<abbr title=\"机器学习的一个分支，它基于人工神经网络。\">深度学习</abbr>模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\n",
      "\n",
      "主要应用领域包括：\n",
      "\n",
      "1. **<abbr title=\"计算机视觉中的一个核心任务，旨在识别和分类图像中的对象。\">图像识别</abbr>**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\n",
      "2. **目标检测**：用于在图像中定位和识别多个对象。\n",
      "3. **图像分割**：将图像划分为不同的区域或对象。\n",
      "4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\n",
      "5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\n",
      "6. **自动驾驶**：用于识别道路标志、行人和其他车辆。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "# 修正点：直接从 pydantic 库导入 BaseModel 和 Field\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. 准备工作 ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. 定义期望的输出结构 (使用 Pydantic V2) ---\n",
    "class TerminologyInAnswer(BaseModel):\n",
    "    \"\"\"一个包含标准答案和其中使用的技术术语的结构化模型。\"\"\"\n",
    "    answer: str = Field(description=\"对用户问题的详细、准确的回答。\")\n",
    "    standard_terms_used: List[str] = Field(\n",
    "        description=\"在回答中明确使用到的、来自官方词库的标准技术术语列表。\",\n",
    "        example=[\"卷积神经网络\", \"图像识别\"]\n",
    "    )\n",
    "\n",
    "# --- 3. 创建一个结构化输出链 ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个精通AI技术的专家。请根据用户的问题，以结构化的形式提供答案。\"),\n",
    "    (\"human\", \"请解释一下什么是CNN，以及它的主要应用领域。\")\n",
    "])\n",
    "\n",
    "# 使用 .with_structured_output() 将LLM与我们的Pydantic V2模型绑定\n",
    "# 现在它会默认使用最高效的json_schema模式，而不会产生警告\n",
    "structured_llm_chain = prompt | llm.with_structured_output(TerminologyInAnswer)\n",
    "\n",
    "# --- 4. 执行链并查看结果 ---\n",
    "print(\"--- 正在执行结构化输出链 ---\")\n",
    "structured_response = structured_llm_chain.invoke({})\n",
    "\n",
    "print(\"\\n--- LLM返回的结构化对象 ---\")\n",
    "print(structured_response)\n",
    "\n",
    "print(\"\\n--- 对结果的分析 ---\")\n",
    "print(f\"回答内容: {structured_response.answer}\")\n",
    "print(f\"模型确认使用的标准术语: {structured_response.standard_terms_used}\")\n",
    "\n",
    "if \"卷积神经网络\" in structured_response.standard_terms_used:\n",
    "    print(\"验证通过：答案中正确地使用了标准术语'卷积神经网络'。\")\n",
    "\n",
    "\n",
    "# --- 5. 内容增强：自动添加术语解释 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 开始执行内容增强 ---\")\n",
    "\n",
    "# 假设我们有这样一个简化的术语词库\n",
    "glossary = {\n",
    "    \"卷积神经网络\": \"一种特殊的深度学习模型，擅长处理图像数据。\",\n",
    "    \"深度学习\": \"机器学习的一个分支，它基于人工神经网络。\",\n",
    "    \"图像识别\": \"计算机视觉中的一个核心任务，旨在识别和分类图像中的对象。\"\n",
    "}\n",
    "\n",
    "# 获取LLM生成的答案文本\n",
    "llm_answer_text = structured_response.answer\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    在文本中查找标准术语，并为其添加Markdown格式的悬浮注释。\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # 创建带注释的Markdown/HTML格式\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # 替换文本中的术语\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# 执行内容增强\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- 内容增强后的最终输出 (在支持HTML的Markdown渲染器中查看) ---\")\n",
    "print(final_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 生成后处理：验证与内容增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 内容增强后的输出 ---\n",
      "<abbr title=\"基于海量数据训练的、参数规模巨大的语言模型。\">大型语言模型</abbr>通常基于<abbr title=\"一种基于自注意力机制的神经网络结构，在NLP领域取得巨大成功。\">Transformer架构</abbr>，而<abbr title=\"一种特殊的深度学习模型，擅长处理图像数据。\">卷积神经网络</abbr>则在图像处理方面是主流。\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有这样一个简化的术语词库\n",
    "glossary = {\n",
    "    \"卷积神经网络\": \"一种特殊的深度学习模型，擅长处理图像数据。\",\n",
    "    \"Transformer架构\": \"一种基于自注意力机制的神经网络结构，在NLP领域取得巨大成功。\",\n",
    "    \"大型语言模型\": \"基于海量数据训练的、参数规模巨大的语言模型。\"\n",
    "}\n",
    "\n",
    "# 假设这是LLM返回的、已经验证过的答案文本\n",
    "llm_answer_text = \"大型语言模型通常基于Transformer架构，而卷积神经网络则在图像处理方面是主流。\"\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    在文本中查找标准术语，并为其添加Markdown格式的悬浮注释。\n",
    "    在支持HTML的Markdown渲染器中，这通常会显示为鼠标悬停提示。\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # 创建带注释的Markdown/HTML格式\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # 替换文本中的术语\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# 执行内容增强\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- 内容增强后的输出 ---\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、评估与反馈机制\n",
    "\n",
    "人工抽样评估难以覆盖海量的生成内容，而“LLM即评委”为此提供了一个高效、可扩展的自动化解决方案。其核心是利用一个LLM的理解和推理能力，来评估另一个LLM（或整个RAG系统）的输出质量。\n",
    "\n",
    "使用LCEL构建评估链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在评估【优秀回答】---\n",
      "{'准确性': {'评价': '高', '说明': \"回答中正确使用了'大语言模型'、'Transformer模型'和'卷积神经网络'等标准术语。\"}, '规范性': {'评价': '高', '说明': '回答中避免了使用非官方的、易产生歧义的别名，使用的术语均为官方标准术语。'}, '全面性': {'评价': '高', '说明': '回答在必要时使用了最能表达其意的标准术语，清晰地传达了大语言模型与Transformer模型的关系，以及卷积神经网络的应用领域。'}}\n",
      "\n",
      "--- 正在评估【待改进回答】---\n",
      "{'准确性': {'评价': '不准确', '问题': \"使用了不正确的术语'卷基神经网络'，应为'卷积神经网络'。\"}, '规范性': {'评价': '不规范', '问题': \"使用了非官方的术语'大模型'，应为'大语言模型'或其别名'LLM'。\"}, '全面性': {'评价': '不全面', '问题': \"未使用最能表达其意的标准术语'Transformer模型'，而是使用了非标准的'变换器架构'。\"}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. 准备工作 ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "evaluator_llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. 定义评估结果的结构化模型 ---\n",
    "class TerminologyEvaluation(BaseModel):\n",
    "    \"\"\"用于评估术语一致性的结构化模型。\"\"\"\n",
    "    consistency_score: int = Field(description=\"一个从1到5的分数，5代表完全一致，1代表严重不一致。\")\n",
    "    is_consistent: bool = Field(description=\"布尔值，是否整体上符合术语规范。\")\n",
    "    reasoning: str = Field(description=\"对评分的详细解释，指出做得好的地方和存在的问题。\")\n",
    "    suggestions_for_improvement: List[str] = Field(description=\"为改进答案中的术语使用提出的具体建议。\")\n",
    "\n",
    "# --- 3. 构建评估链 ---\n",
    "# 创建一个专门的评估提示\n",
    "evaluation_prompt_template = \"\"\"\n",
    "你是一个严格的AI技术文档质量评估员，你的核心任务是评估一段回答在术语使用上的一致性和准确性。\n",
    "\n",
    "**评估标准:**\n",
    "1.  **准确性**: 是否正确地使用了标准术语？\n",
    "2.  **规范性**: 是否避免了使用非官方的、易产生歧义的别名？\n",
    "3.  **全面性**: 是否在必要时使用了最能表达其意的标准术语？\n",
    "\n",
    "**权威术语词库 (部分):**\n",
    "- 卷积神经网络 (别名: CNN)\n",
    "- Transformer模型 (别名: Transformer)\n",
    "- 大语言模型 (别名: LLM)\n",
    "\n",
    "**待评估的回答:**\n",
    "{answer_text}\n",
    "\n",
    "请根据以上标准和词库，以JSON格式输出你的评估结果。\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(evaluation_prompt_template)\n",
    "parser = JsonOutputParser(pydantic_object=TerminologyEvaluation)\n",
    "\n",
    "# 使用LCEL构建评估链\n",
    "evaluation_chain = prompt | evaluator_llm | parser\n",
    "\n",
    "# --- 4. 执行评估 ---\n",
    "\n",
    "# 案例一：一个使用规范的回答\n",
    "good_answer = \"大语言模型（LLM）是基于Transformer模型构建的，而卷积神经网络（CNN）则在图像领域应用广泛。\"\n",
    "\n",
    "# 案例二：一个使用了非标准术语的回答\n",
    "bad_answer = \"大模型是基于变换器架构的，而卷基神经网络在图片处理上很强。\"\n",
    "\n",
    "print(\"--- 正在评估【优秀回答】---\")\n",
    "good_evaluation_result = evaluation_chain.invoke({\"answer_text\": good_answer})\n",
    "print(good_evaluation_result)\n",
    "\n",
    "\n",
    "print(\"\\n--- 正在评估【待改进回答】---\")\n",
    "bad_evaluation_result = evaluation_chain.invoke({\"answer_text\": bad_answer})\n",
    "print(bad_evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：术语一致性优化路线图\n",
    "\n",
    "经过以上各阶段的详细探讨，从数据预处理到最终的评估反馈，我们已经全面构建了保障术语一致性的技术体系。\n",
    "\n",
    "为了更直观地理解各项技术的定位与优先级，我们将整个优化策略总结为以下分级路线图，为不同阶段的RAG系统建设提供实践指引。\n",
    "\n",
    "| 优化层级                | 核心技术与解决方案                                                                 |\n",
    "|-------------------------|------------------------------------------------------------------------------------|\n",
    "| 基础核心 (Foundation)    | 术语词库构建、术语抽取、预处理标准化、术语嵌入与向量索引                             |\n",
    "| 关键增强 (Key Enhancement)| 混合检索 (BM25 + 向量)、查询扩展 (MultiQuery)、假设性文档嵌入 (HyDE)、交叉编码器重排序 |\n",
    "| 辅助优化 (Auxiliary Optimization) | 领域专用嵌入模型微调、上下文感知分块                                               |\n",
    "| 生成控制 (Generation Control)     | 提示工程、结构化输出、输出解析与修复                                               |\n",
    "| 长期保障 (Long-term Assurance)    | LLM即评委 (LLM-as-a-Judge)、用户反馈闭环、日志审计与分析                            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
